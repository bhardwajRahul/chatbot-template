# LLM Chatbot Backend Framework

- A modular backend framework for building AI chat applications powered by large language models (LLMs), using FastAPI and MongoDB
- A flexible template that incorporates the latest techniques and best practices for building production-ready chatbots

ðŸ“š **Developer docs available in the [docs/](./docs/) folder.**


## Project Structure

The project is organized using a layered Bot-Brain architecture with a modular Components Layer:

```
â”œâ”€â”€ api/                  # API layer with FastAPI
â”‚   â”œâ”€â”€ v1/               # Versioned APIs
â”‚   â”‚   â”œâ”€â”€ chat.py       # Chat endpoints
â”‚   â”‚   â”œâ”€â”€ health.py     # Health check endpoints
â”‚   â”‚   â””â”€â”€ __init__.py   # API initialization
â”‚   â”œâ”€â”€ middleware/       # API middleware components 
â”‚   â”œâ”€â”€ app.py            # API application configuration
â”‚   â””â”€â”€ routes.py         # API route definitions
â”œâ”€â”€ src/                  # Source code
â”‚   â”œâ”€â”€ bot.py            # Main Bot class (orchestrates components)
â”‚   â”œâ”€â”€ chat_engine.py    # Chat engine implementation
â”‚   â”œâ”€â”€ reasoning/        # Reasoning components
â”‚   â”‚   â”œâ”€â”€ brains/       # LLM-powered reasoning
â”‚   â”‚   â”‚   â”œâ”€â”€ base.py   # BaseBrain abstract class
â”‚   â”‚   â”‚   â”œâ”€â”€ brain_factory.py # Factory to choose correct brain
â”‚   â”‚   â”‚   â”œâ”€â”€ services/ # Brain service implementations
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ agent_brain.py # Agent-based brain implementation
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ llm_brain.py # Base LLM brain implementation
â”‚   â”‚   â”‚   â””â”€â”€ __init__.py # Brain module initialization
â”‚   â”œâ”€â”€ components/       # Components Layer: modular, interchangeable units
â”‚   â”‚   â”œâ”€â”€ llms/         # LLM client components
â”‚   â”‚   â”‚   â”œâ”€â”€ base.py   # BaseLLMClient abstract interface
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_factory.py # Factory for LLM clients
â”‚   â”‚   â”‚   â””â”€â”€ clients/  # Implementations for different LLM providers
â”‚   â”‚   â”‚       â”œâ”€â”€ openai_client.py  # OpenAI API client
â”‚   â”‚   â”‚       â”œâ”€â”€ azure_openai_client.py # Azure OpenAI API client
â”‚   â”‚   â”‚       â”œâ”€â”€ llamacpp_client.py # LlamaCpp client
â”‚   â”‚   â”‚       â””â”€â”€ vertex_client.py # Google Vertex AI client
â”‚   â”‚   â”œâ”€â”€ memory/       # Memory persistence components
â”‚   â”‚   â”‚   â”œâ”€â”€ base.py   # BaseChatbotMemory abstract interface
â”‚   â”‚   â”‚   â”œâ”€â”€ memory_factory.py # Factory for memory implementations
â”‚   â”‚   â”‚   â””â”€â”€ clients/  # Memory storage implementations
â”‚   â”‚   â”‚       â”œâ”€â”€ in_memory.py # In-memory implementation
â”‚   â”‚   â”‚       â””â”€â”€ mongodb_memory.py # MongoDB implementation
â”‚   â”‚   â””â”€â”€ tools/        # Tool components for agent capabilities
â”‚   â”‚       â”œâ”€â”€ base.py   # BaseTool abstract interface
â”‚   â”‚       â”œâ”€â”€ serp.py   # Search tool implementation
â”‚   â”‚       â””â”€â”€ __init__.py # Tool components initialization
â”‚   â””â”€â”€ common/           # Shared utilities and models
â”‚       â”œâ”€â”€ config.py     # Configuration management
â”‚       â”œâ”€â”€ models.py     # Data models
â”‚       â”œâ”€â”€ schemas.py    # Schema definitions
â”‚       â”œâ”€â”€ exceptions.py # Custom exceptions
â”‚       â””â”€â”€ logging.py    # Logging setup
â”œâ”€â”€ configuration/        # Configuration files and settings
â”œâ”€â”€ tests/                # Test directory
â”‚   â”œâ”€â”€ api/              # API tests
â”‚   â”‚   â””â”€â”€ test_chat.py  # Chat endpoint tests
â”‚   â”œâ”€â”€ llm_clients/      # LLM client tests
â”‚   â”‚   â””â”€â”€ test_base_llm_client.py # Base LLM client tests
â”‚   â”œâ”€â”€ tools/            # Tool tests
â”‚   â”‚   â”œâ”€â”€ test_base_tool.py # Base tool tests
â”‚   â”‚   â””â”€â”€ test_serp_tool.py # Search tool tests
â”‚   â”œâ”€â”€ conftest.py       # Test configuration
â”‚   â””â”€â”€ test_cli.py       # CLI tests
â”œâ”€â”€ docs/                 # Documentation
â”œâ”€â”€ app.py                # FastAPI application
â”œâ”€â”€ cli.py                # Command-line interface for local testing
â”œâ”€â”€ Dockerfile            # Docker configuration
â””â”€â”€ requirements.txt      # Project dependencies
```

## Architecture

The application follows a Bot-Brain architecture with standardized interfaces:

1. **Bot Layer** - Handles request processing and message coordination
   - A single `Bot` class delegates reasoning to a pluggable `Brain`
   - Manages memory and conversation state

2. **Reasoning Layer** - Responsible for LLM-based reasoning
   - **Brains**: Each brain implementation encapsulates reasoning logic
     - Created by a factory based on configuration
     - Follows a common interface defined by `BaseBrain`

3. **Components Layer** - Reusable components for building chatbot systems
   - Modular design with standardized interfaces
   - Manages LLM client implementations through the `BaseLLMClient` interface
   - Handles memory persistence with `BaseChatbotMemory` implementations
   - Manages tool integrations through the `BaseTool` interface and `ToolProvider`
   - Enables easy extension and replacement of functional components


## Bot-Brain Architecture Diagram

```mermaid
graph TD
    %% Entry
    UserInput["User Input"] --> Bot

    %% Bot Layer
    Bot --> BrainFactory
    BrainFactory --> BrainInstance["Brain (via config)"]
    Bot --> Memory
    Bot --> ToolProvider["Tool Provider"]
    
    %% Reasoning Layer
    BrainInstance -->|calls| ComponentsLayer["Components Layer"]
    
    %% Components Layer
    subgraph ComponentsLayer["Components Layer"]
        LLMClient["LLM Clients"]
        MemoryImplementations["Memory Implementations"]
        ToolsManagement["Tool Management"]
    end
    
    BrainInstance -->|uses| LLMClient
    BrainInstance -->|uses via| ToolsManagement
    BrainInstance -->|reads/writes via| MemoryImplementations
    
    %% LLM Layer
    LLMClient -->|calls| ExternalAPI["External LLM API"]
    
    %% Tool Layer
    ToolsManagement -->|manages| SearchTool["Search Tool"]
    ToolsManagement -->|manages| OtherTools["Other Tools"]
    
    %% Memory Layer
    MemoryImplementations -->|implementations| StorageOptions["MongoDB / In-Memory"]
    
    %% Configuration
    Config["Configuration"] --> BrainFactory
    Config --> ComponentsLayer
```

### Brain Composition Detail

```mermaid
graph TD
    subgraph Brain
        BrainNode["Brain"] --> ChainManager
        BrainNode --> BrainTools["Brain Tools"]
        BrainNode --> BrainMemory["Brain Memory"]
        ChainManager --> LLMClientNode["LLM Client"]
        ChainManager --> ToolCalls["Tool Calls"]
    end
```


## Getting Started

### Prerequisites

- Python 3.9+
- MongoDB (for persistent memory)

### Installation

1. Clone the repository
2. Install dependencies:
   ```
   pip install -r requirements.txt
   ```
3. Set up environment variables:
   ```
   # For OpenAI
   OPENAI_API_KEY=your_openai_key
   MODEL_TYPE=OPENAI
   
   # For Azure OpenAI
   AZURE_API_KEY=your_azure_openai_key
   AZURE_ENDPOINT=your_azure_endpoint
   AZURE_DEPLOYMENT_NAME=your_deployment_name
   MODEL_TYPE=AZUREOPENAI
   
   # Database
   MONGO_URI=your_mongodb_uri
   ```

### Running the Application

```
python app.py
```

The API will be available at http://localhost:8080

### Using the CLI Interface

For quick testing without running the API server, you can use the command-line interface:

```bash
# Basic usage
python cli.py

# Specify a model type
python cli.py --model llama

# Use Azure OpenAI
python cli.py --model azureopenai

# Specify a custom conversation ID
python cli.py --conversation-id my_session_123
```

Exit the CLI by typing 'exit', 'quit', or pressing Ctrl+C.
