# LLM Backend Framework

A modular backend framework for building AI applications with large language models (LLMs), FastAPI, and MongoDB.

ðŸ“š **Developer docs available in the [docs/](./docs/) folder.**

ðŸ“‹ **Check out our [improvement suggestions](./docs/improvement_suggestions.md) for future development.**

## Project Structure

The project is organized using the Bot-Brain architecture with standardized interfaces:

```
â”œâ”€â”€ api/                  # API layer with FastAPI
â”‚   â”œâ”€â”€ v1/               # Versioned APIs
â”‚   â”‚   â”œâ”€â”€ chat.py       # Chat endpoints
â”‚   â”‚   â”œâ”€â”€ health.py     # Health check endpoints
â”‚   â”‚   â””â”€â”€ __init__.py   # API initialization
â”‚   â”œâ”€â”€ middleware/       # API middleware components 
â”‚   â”œâ”€â”€ app.py            # API application configuration
â”‚   â”œâ”€â”€ models.py         # API data models
â”‚   â””â”€â”€ routes.py         # API route definitions
â”œâ”€â”€ src/                  # Source code
â”‚   â”œâ”€â”€ bot.py            # Main Bot class
â”‚   â”œâ”€â”€ reasoning/        # Reasoning components
â”‚   â”‚   â”œâ”€â”€ brains/       # LLM-powered reasoning
â”‚   â”‚   â”‚   â”œâ”€â”€ base.py   # BaseBrain abstract class
â”‚   â”‚   â”‚   â”œâ”€â”€ brain_factory.py # Factory to choose correct brain
â”‚   â”‚   â”‚   â”œâ”€â”€ services/ # Brain service implementations
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ openai_brain.py # Brain using OpenAI LLM
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ llama_brain.py # Brain using LlamaCpp
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ azure_openai_brain.py # Brain using Azure OpenAI
â”‚   â”‚   â”‚   â””â”€â”€ __init__.py # Brain module initialization
â”‚   â”‚   â””â”€â”€ chain_manager.py # Chain manager for reasoning flows
â”‚   â”œâ”€â”€ memory/           # Conversation memory modules
â”‚   â”‚   â”œâ”€â”€ __init__.py   # Memory module initialization
â”‚   â”‚   â”œâ”€â”€ base_memory.py # BaseChatbotMemory abstract class
â”‚   â”‚   â”œâ”€â”€ custom_memory.py # In-memory implementation
â”‚   â”‚   â””â”€â”€ mongodb_memory.py # MongoDB implementation
â”‚   â”œâ”€â”€ llms/             # LLM abstraction layer
â”‚   â”‚   â”œâ”€â”€ base.py       # BaseLLMClient abstract class
â”‚   â”‚   â””â”€â”€ clients/      # LLM-specific client implementations
â”‚   â”‚       â”œâ”€â”€ openai_client.py  # OpenAI API client
â”‚   â”‚       â”œâ”€â”€ azure_openai_client.py # Azure OpenAI API client
â”‚   â”‚       â”œâ”€â”€ llamacpp_client.py # LlamaCpp client
â”‚   â”‚       â””â”€â”€ vertex_client.py # Google Vertex AI client
â”‚   â”œâ”€â”€ common/           # Shared utilities and models
â”‚   â”‚   â”œâ”€â”€ objects.py    # Shared data models
â”‚   â”‚   â””â”€â”€ config.py     # Configuration management
â”‚   â””â”€â”€ tools/            # Tools for agent capabilities
â”‚       â”œâ”€â”€ base.py       # BaseTool abstract class
â”‚       â”œâ”€â”€ serp.py       # Search tool implementation
â”‚       â””â”€â”€ __init__.py   # Tools initialization
â”œâ”€â”€ infrastructure/       # Infrastructure concerns
â”‚   â”œâ”€â”€ db/               # Database clients
â”‚   â”‚   â””â”€â”€ mongodb.py    # MongoDB client
â”‚   â””â”€â”€ di/               # Dependency injection
â”œâ”€â”€ tests/                # Test directory
â”‚   â”œâ”€â”€ api/              # API tests
â”‚   â”œâ”€â”€ bot/              # Bot tests
â”‚   â”œâ”€â”€ reasoning/        # Reasoning tests
â”‚   â”‚   â””â”€â”€ brains/       # Brain tests
â”‚   â”œâ”€â”€ memory/           # Memory tests 
â”‚   â”œâ”€â”€ tools/            # Tool tests
â”‚   â””â”€â”€ llms/             # LLM client tests
â”œâ”€â”€ docs/                 # Documentation
â”‚   â”œâ”€â”€ architecture.md   # Architecture guide
â”‚   â”œâ”€â”€ extending.md      # Extension guide
â”‚   â”œâ”€â”€ api.md            # API documentation
â”‚   â”œâ”€â”€ folder_structure.md # Folder structure guide
â”‚   â””â”€â”€ improvement_suggestions.md # Suggestions for future improvements
â”œâ”€â”€ app.py                # FastAPI application
â”œâ”€â”€ cli.py                # Command-line interface for local testing
â”œâ”€â”€ Dockerfile            # Docker configuration
â””â”€â”€ requirements.txt      # Project dependencies
```

## Architecture

The application follows a Bot-Brain architecture with standardized interfaces:

1. **Bot Layer** - Handles request processing and message coordination
   - A single `Bot` class delegates reasoning to a pluggable `Brain`
   - Manages memory and conversation state

2. **Reasoning Layer** - Responsible for LLM-based reasoning
   - **Brains**: Each brain implementation encapsulates reasoning logic
     - Created by a factory based on configuration
     - Follows a common interface defined by `BaseBrain`
   - **Chains**: Structured reasoning flows for specific tasks

3. **LLM Clients** - Standardized interface for LLM providers
   - Follows `BaseLLMClient` interface
   - Supports chat and completion operations
   - Multiple implementations for different providers (OpenAI, Azure OpenAI, etc.)

4. **Tools** - Standardized interface for agent tools
   - Follows `BaseTool` interface
   - Consistent schema handling with OpenAI tool format
   - Supports a variety of capabilities (search, etc.)

## Bot-Brain Architecture Diagram

```mermaid
graph TD
    %% Entry
    UserInput["User Input"] --> Bot

    %% Bot Layer
    Bot --> BrainFactory
    BrainFactory --> BrainInstance["Brain (via config)"]
    Bot --> Memory
    
    %% Reasoning Layer
    BrainInstance -->|calls| LLMClient
    BrainInstance -->|may use| Tools
    BrainInstance -->|reads/writes| Memory
    
    %% LLM Layer
    LLMClient -->|calls| ExternalAPI["External LLM API"]
    
    %% Tool Layer
    Tools --> SearchTool["Search Tool"]
    Tools --> OtherTools["Other Tools"]
    
    %% Memory Layer
    Memory -->|implementations| StorageOptions["MongoDB / In-Memory"]
    
    %% Configuration
    Config["Configuration"] --> BrainFactory
    Config --> LLMClient
```

### Brain Composition Detail

```mermaid
graph TD
    subgraph Brain
        BrainNode["Brain"] --> ChainManager
        BrainNode --> BrainTools["Brain Tools"]
        BrainNode --> BrainMemory["Brain Memory"]
        ChainManager --> LLMClientNode["LLM Client"]
        ChainManager --> ToolCalls["Tool Calls"]
    end
```

## Implementation Notes

The codebase implements the Bot-Brain architecture with standardized interfaces:

### LLM Client Interface

All LLM clients implement a standard interface:

```python
class BaseLLMClient(ABC):
    @abstractmethod
    def chat(self, messages: List[Dict[str, str]], **kwargs: Any) -> str:
        """Send a chat message to the LLM and get a response."""
        pass
    
    @abstractmethod
    def complete(self, prompt: str, **kwargs: Any) -> str:
        """Send a completion prompt to the LLM and get a response."""
        pass
    
    @abstractmethod
    def get_model_info(self) -> Dict[str, Any]:
        """Get information about the LLM model."""
        pass
```

### Tool Interface

Tools follow a standard interface for extension:

```python
class BaseTool(ABC):
    def __init__(self, name: str, description: str):
        self.name = name
        self.description = description
    
    @abstractmethod
    def run(self, input_data: Any) -> Any:
        """Execute the tool with the given input."""
        pass
    
    @abstractmethod
    def get_parameters_schema(self) -> Dict[str, Any]:
        """Get the JSON schema for the tool's parameters."""
        pass
    
    def to_openai_tool(self) -> Dict[str, Any]:
        """Convert the tool to an OpenAI tool format."""
        return {
            "type": "function",
            "function": {
                "name": self.name,
                "description": self.description,
                "parameters": self.get_parameters_schema(),
            }
        }
```

### Brain Implementations

Brain implementations are now under the reasoning/brains/services module:

```python
class BaseBrain(ABC):
    @abstractmethod
    def think(self, query: str, context: Optional[Dict[str, Any]] = None) -> str:
        """Process the query and return a response."""
        pass
    
    @abstractmethod
    def reset(self) -> None:
        """Reset the brain's state."""
        pass
```

Different implementations include:
- `OpenAIBrain` - Brain using OpenAI models for reasoning
- `LlamaBrain` - Brain using LlamaCpp for local reasoning
- `AzureOpenAIBrain` - Brain using Azure OpenAI models for reasoning

### Testing

The `tests` directory includes comprehensive tests:

- Mock implementations of `BaseLLMClient` and `BaseTool` for testing
- Unit tests for tools, LLM clients, and other components
- API tests for the FastAPI endpoints

## Status and Extensions

The codebase has been refactored to use standardized interfaces:

1. **âœ… Standardized LLM Client Interface**:
   - All LLM clients implement `BaseLLMClient`
   - Consistent methods for chat, completion

2. **âœ… Standardized Tool Interface**:
   - All tools implement `BaseTool`
   - Consistent schema definition and execution

3. **âœ… Improved Directory Structure**:
   - Renamed directories for clarity (llms â†’ llm_clients, chains â†’ reasoning)
   - Better organization with brains under reasoning/brains/services
   - Consolidated test structure

4. **âœ… Enhanced Extensibility**:
   - New LLM clients can be added by implementing `BaseLLMClient`
   - New tools can be added by implementing `BaseTool`
   - Brain implementations can be swapped without changing the Bot

5. **âœ… Multiple LLM Providers Support**:
   - OpenAI API integration
   - Azure OpenAI API integration
   - Local LLM support via LlamaCpp
   - Google Vertex AI integration

## Getting Started

### Prerequisites

- Python 3.9+
- MongoDB (for persistent memory)

### Installation

1. Clone the repository
2. Install dependencies:
   ```
   pip install -r requirements.txt
   ```
3. Set up environment variables:
   ```
   # For OpenAI
   OPENAI_API_KEY=your_openai_key
   MODEL_TYPE=OPENAI
   
   # For Azure OpenAI
   AZURE_API_KEY=your_azure_openai_key
   AZURE_ENDPOINT=your_azure_endpoint
   AZURE_DEPLOYMENT_NAME=your_deployment_name
   MODEL_TYPE=AZUREOPENAI
   
   # Database
   MONGO_URI=your_mongodb_uri
   ```

### Running the Application

```
python app.py
```

The API will be available at http://localhost:8080

### Using the CLI Interface

For quick testing without running the API server, you can use the command-line interface:

```bash
# Basic usage
python cli.py

# Specify a model type
python cli.py --model llama

# Use Azure OpenAI
python cli.py --model azureopenai

# Specify a custom conversation ID
python cli.py --conversation-id my_session_123
```

Exit the CLI by typing 'exit', 'quit', or pressing Ctrl+C.

## Extending the Framework

### Adding a New LLM Client

1. Create a new file in `src/llms/clients/`
2. Implement the `BaseLLMClient` interface
3. Register the client in the factory if needed

### Adding a New Tool

1. Create a new file in `src/tools/`
2. Implement the `BaseTool` interface
3. Add the tool to the tool registry

### Adding a New Brain

1. Create a new file in `src/reasoning/brains/services/`
2. Implement the `BaseBrain` interface
3. Update the brain factory if needed